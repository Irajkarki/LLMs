{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c235d08c-f4ed-422d-b7ec-104ba8cf2300",
   "metadata": {},
   "source": [
    "In this section, we implement data loader that fetches the input-target pairs using the sliding window approach. \n",
    "At first, we tokenize the the verdict text using BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369aad38-7f07-4c1f-a229-080356e1262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0c1b8e-7e04-48bf-bd57-340554e5672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10353d46-6a59-4f4f-a2c5-635ad03530e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt','r',encoding = 'utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04fdd31-bf9a-4e26-b0e0-55a9d561fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2392d2d0-41b9-4b5b-a7b1-a7ecd411d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "#Context_size determines how many tokens are included in the input\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f'x: {x}')\n",
    "print(f'y:      {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54382e02-03b6-4db5-8b6f-83b6f1e700e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "#processing the input along with targets, which are the inputs shifted by one position, we can create a next word prediction tasks as follows:\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    \n",
    "    print(context,'---->',desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a7b415-7489-4910-a99a-8880446fc679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context),'--->', tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262f9fab-a485-4b14-858d-86bccfbcbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9832e729-160f-49cc-b0a8-386c708b36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {'<|endoftext|>'})\n",
    "\n",
    "        #Use the sliding window to chunk the bookinto overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length +1 ]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b861434-0cd6-4234-b337-5539f9f554ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataLoader\n",
    "\n",
    "def create_dataloader_V1(txt, batch_size = 4, max_length = 256 ,stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    #Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    #Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    #Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "736f2db2-0d22-44ed-b7be-25fc3a4e593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let test the dataloader with a batch size of 1 and context size of 4,\n",
    "# this will develop an intuition of how the GPTDatasetV1 class and create_dataloader_V1 function work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d3ed7bf-c6f9-4e7e-9e94-0262df0b3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', encoding = 'utf-8') as f:\n",
    "    raw_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69912442-05b8-4e86-9537-b6af7c00193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "#convert dataloader into python iterator to fetch the next entry via Python's built-in next() function\n",
    "\n",
    "dataloader = create_dataloader_V1(raw_text, batch_size =1, max_length = 4, stride = 1, shuffle = False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec354bd2-6dad-494d-a037-4ecc8e0ed83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c723c1f7-a0a3-4cff-8c89-dcfe3f6c0948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_V1(raw_text, batch_size =8, max_length = 4, stride = 4, shuffle = False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "391e1203-eda3-461e-9257-34a47c71c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  287,   262,  6001,   286],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [  314,  2138,  1807,   340],\n",
      "        [  393, 28537,  2014,   198],\n",
      "        [  286,   465, 13476,     1],\n",
      "        [  262,  1466,  1444,   340],\n",
      "        [ 9074,    13, 46606,   536],\n",
      "        [ 4842,  1650,   353,   438],\n",
      "        [48422,   540,   450,    67],\n",
      "        [ 1781,   340,   338,  1016]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [  262,  6001,   286,   465],\n",
      "        [ 5710,   465, 12036,    11],\n",
      "        [   11,   290,  4920,  2241],\n",
      "        [  319,   262, 34686, 41976],\n",
      "        [ 2138,  1807,   340,   561],\n",
      "        [28537,  2014,   198,   198],\n",
      "        [  465, 13476,     1,   438],\n",
      "        [ 1466,  1444,   340,    13],\n",
      "        [   13, 46606,   536,  5469],\n",
      "        [ 1650,   353,   438,  2934],\n",
      "        [  540,   450,    67,  3299],\n",
      "        [  340,   338,  1016,   284]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_V1(raw_text, batch_size =16, max_length = 4, stride = 8, shuffle = False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee5909-3d77-4471-839f-74aa406a6a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
