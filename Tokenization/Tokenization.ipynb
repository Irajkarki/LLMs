{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae4394e-188a-4db8-8112-9e624e3dc4b9",
   "metadata": {},
   "source": [
    "#Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b8e32d-1a77-497e-a348-f7bf0b7765dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chracter: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt','r',encoding = 'utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print('Total number of chracter:', len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01576c42-cb33-41cc-924b-e3e4e6d3d1e5",
   "metadata": {},
   "source": [
    "Splitting the texts using by importing the re library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f5b678-d7de-4dd3-a9f3-0519b0fc50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world!.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world!. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) #'\\s' split the text in white spacing\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c98498f3-2083-4c82-916d-512aa8037a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "#Splitting the text in comma and fullstop along with white spacing\n",
    "result = re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cee778-f4df-415f-a905-ccddcded5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "#remove the white spacing\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ac671-8e6d-46a0-be67-b1a2fb137088",
   "metadata": {},
   "source": [
    "we can remove the white spacing. if we remove the it will recude the computation power and memory usage. but in some cases we should not remove the white spacing such as while the input is python code(It is sensitive).keeping whitespaces can be useful if we train models that are sensitive to the exact structue of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a0b57b-6371-4103-b03f-11a89049533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#splitting the text in all special characters\n",
    "text =  \"Hello, world!. Is this -- a test?\"\n",
    "result = re.split(r'([!,.:_?\\'()/]|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb7d9ae5-ccd4-477f-a3de-f44a1a8da057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring']\n"
     ]
    }
   ],
   "source": [
    "#now using re in the raw text\n",
    "preprocessed = re.split(r'([!,.:_?\\'()/]|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78472606-bdd6-4235-aa3f-6133b419fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48d98999-3e8d-482e-9b99-a2204c5b3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: creating token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cfec7c1-38eb-4a57-a679-6f4522d2a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n"
     ]
    }
   ],
   "source": [
    "all_words =  sorted(set(preprocessed))\n",
    "vocab_size =  len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4991d6ef-2e90-4caf-938e-2a5680cff14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:  integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6360688b-68cc-4dec-9d74-4872f1da4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"If', 15)\n",
      "('\"It', 16)\n",
      "('\"Jack', 17)\n",
      "('\"Money', 18)\n",
      "('\"Moon-dancers\"', 19)\n",
      "('\"Mr', 20)\n",
      "('\"Mrs', 21)\n",
      "('\"My', 22)\n",
      "('\"Never', 23)\n",
      "('\"Of', 24)\n",
      "('\"Oh', 25)\n",
      "('\"Once', 26)\n",
      "('\"Only', 27)\n",
      "('\"Or', 28)\n",
      "('\"That', 29)\n",
      "('\"The', 30)\n",
      "('\"Then', 31)\n",
      "('\"There', 32)\n",
      "('\"This', 33)\n",
      "('\"We', 34)\n",
      "('\"Well', 35)\n",
      "('\"What', 36)\n",
      "('\"When', 37)\n",
      "('\"Why', 38)\n",
      "('\"Yes', 39)\n",
      "('\"You', 40)\n",
      "('\"but', 41)\n",
      "('\"deadening', 42)\n",
      "('\"dragged', 43)\n",
      "('\"effects\";', 44)\n",
      "('\"interesting\"', 45)\n",
      "('\"lift', 46)\n",
      "('\"obituary\"', 47)\n",
      "('\"strongest', 48)\n",
      "('\"strongly\"', 49)\n",
      "('\"sweetly\"', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, items in enumerate(vocab.items()):\n",
    "    print(items)\n",
    "    if i >=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb8fc259-92fb-4610-9d05-273b2a0c9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([!,.:_?\\'()/]|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids= [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #Replace the space before puncuation\n",
    "        text = re.sub(r'([!,.:_?\\'()/]|--|\\s)', r'\\1', text)\n",
    "                     \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9b9c35a-a5e6-4748-a8e5-d6f22e9bb4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 51, 346, 857, 666, 1053, 1016, 1166, 55, 240, 92, 51, 1101, 183, 1002, 1166, 759, 676, 735, 1157, 56, 1]\n"
     ]
    }
   ],
   "source": [
    "#Now creating the instance of the class\n",
    "\n",
    "Tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\" \"I'd rather like to tell you--because I've always suspected you of loathing my work.\" \"\"\"\n",
    "\n",
    "ids = Tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc6d216b-ef26-4020-86b8-8cae2c359016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I \\' d rather like to tell you -- because I \\' ve always suspected you of loathing my work . \"'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoding\n",
    "Tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8efa0-fbd1-4893-a5bc-833b62d0903e",
   "metadata": {},
   "source": [
    "Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95421197-bac9-46d5-af61-0720d40e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f2ab6ab-776f-4cf9-8c55-45471a59e741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1172"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e31a1d1-3f70-4c8f-ada7-464d8cc7108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1167)\n",
      "('your', 1168)\n",
      "('yourself', 1169)\n",
      "('<|endoftext|>', 1170)\n",
      "('<|unk|>', 1171)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3203a92d-dc34-4148-a1df-dfc30b8447e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([!,.:_?\\'()/]|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed  \n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'([!,.:_?\\'()/]|--|\\s)',r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5919adca-b6d8-4960-9ae4-107abccf4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5cf780c0-db59-4414-aeec-59e0119878f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1171, 54, 390, 1166, 666, 1012, 58, 1171, 1025, 994, 1021, 759, 1171, 56]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1bd3c5d3-eb5e-44ba-a935-cadaef54701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> , do you like tea ? <|unk|> the sunlit terraces of <|unk|> .\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d86a0a-429f-43f7-b445-d58573942118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
